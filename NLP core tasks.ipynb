{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook summarizes some core NLP tasks I learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')#small and medium libraries are okay too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sentences\n",
    "doc=nlp(u'I am Azka. I want to learn Machine Learning more. I want to dive deep in Deep Learning. I am passionate about Artificial Intelligence. I want to join this community!.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into sentences.\n",
      "[I am Azka., I want to learn Machine Learning more., I want to dive deep in Deep Learning., I am passionate about Artificial Intelligence., I want to join this community!.]\n",
      "\n",
      "Tokens of a single sentence\n",
      "I\n",
      "want\n",
      "to\n",
      "join\n",
      "this\n",
      "community\n",
      "!\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "sents=[sent for sent in doc.sents]\n",
    "print(f'Splitting into sentences.')\n",
    "print(sents)\n",
    "print('\\nTokens of a single sentence')\n",
    "for tokens in sents[4]:\n",
    "    print(tokens.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google LLC                          ORG            Companies, agencies, institutions, etc.\n",
      "American                            NORP           Nationalities or religious or political groups\n",
      "Google                              ORG            Companies, agencies, institutions, etc.\n",
      "Sundar Pichai                       PERSON         People, including fictional\n",
      "September 4, 1998                   DATE           Absolute or relative dates or periods\n",
      "Menlo Park                          GPE            Countries, cities, states\n",
      "California                          GPE            Countries, cities, states\n",
      "United States                       GPE            Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp(u'Google LLC is an American multinational technology company. CEO of Google is Sundar Pichai.It was founded in September 4, 1998 by Menlo Park, California, United States.')\n",
    "for entities in doc2.ents:\n",
    "    print(f'{entities.text:{35}} {entities.label_:{15}}{str(spacy.explain(entities.label_))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google LLC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " multinational technology company. CEO of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sundar Pichai\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".It was founded in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    September 4, 1998\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Menlo Park\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc2, style='ent', jupyter=True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisa\n",
      "a sleeveless shirt\n",
      "hot soup\n"
     ]
    }
   ],
   "source": [
    "doc3=nlp(u'Lisa is wearing a sleeveless shirt today and eating hot soup.')\n",
    "for chunks in doc3.noun_chunks:\n",
    "    print(chunks.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I                    PRON                 -PRON-\n",
      "was                  AUX                  be\n",
      "sick                 ADJ                  sick\n",
      "I                    PRON                 -PRON-\n",
      "sat                  VERB                 sit\n",
      "I                    PRON                 -PRON-\n",
      "ate                  VERB                 eat\n",
      "I                    PRON                 -PRON-\n",
      "ran                  VERB                 run\n",
      ".                    PUNCT                .\n",
      "I                    PRON                 -PRON-\n",
      "am                   AUX                  be\n",
      "eating               VERB                 eat\n",
      "the                  DET                  the\n",
      "cookies              NOUN                 cookie\n",
      "that                 PRON                 that\n",
      "I                    PRON                 -PRON-\n",
      "baked                VERB                 bake\n",
      "this                 DET                  this\n",
      "morning              NOUN                 morning\n",
      ".                    PUNCT                .\n"
     ]
    }
   ],
   "source": [
    "#reducion of word to its root word\n",
    "doc4=nlp(u'I was sick I sat I ate I ran.I am eating the cookies that I baked this morning.')\n",
    "for token in doc4:\n",
    "    print(f'{token.text:{20}} {token.pos_:{20}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Spacy doesn't support stemming. \n",
    "Using nltk for stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate _____ ate\n",
      "eating _____ eat\n",
      "eater _____ eater\n",
      "eat _____ eat\n",
      "caught _____ caught\n",
      "catch _____ catch\n",
      "slowly _____ slowli\n",
      "gradually _____ gradual\n",
      "mutually _____ mutual\n"
     ]
    }
   ],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "tokens = ['ate','eating','eater','eat','caught','catch','slowly','gradually','mutually']\n",
    "for t in tokens:\n",
    "    print(t+' _____ '+p_stemmer.stem(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a tecnique which follows some rules and cut down last part of words which is not suitable for some words as shown above.\n",
    "Lemmatiation however is intelligent enough to understand the root word for given words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
